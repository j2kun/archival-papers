\relax 
\citation{Getoor2005}
\citation{Gallagher2008}
\citation{Neville2005}
\citation{Caceres2011}
\citation{Miller2014}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{CCK14}
\citation{Aggarwal2011}
\citation{Leskovec2008}
\citation{Rossi2012}
\citation{Xiang2010}
\citation{Gilbert2009}
\citation{Papalexakis2013}
\citation{Tang2009}
\citation{Tang2012}
\citation{Mucha2010}
\citation{Berlingerio2011}
\citation{Rocklin2013}
\citation{Cai2005}
\citation{Balcan2006}
\citation{Balcan2008}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}}
\newlabel{sec:related}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Representation Learning and Clustering}{3}}
\citation{Schapire90}
\citation{Bubeck12}
\citation{Schapire90}
\citation{Bubeck09}
\citation{Arora12}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Boosting and Bandits}{4}}
\citation{Walktrap}
\@writefile{toc}{\contentsline {section}{\numberline {3}The Locally Boosted Graph Aggregation Framework}{5}}
\newlabel{sec:lbga}{{3}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Framework Details}{5}}
\newlabel{sec:framework}{{3.1}{5}}
\citation{Dice1945}
\citation{Jaccard1912}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Quality Measures for Community Detection}{6}}
\newlabel{sec:quality-measures}{{3.2}{6}}
\citation{Wang87}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}LBGA Implementation}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Analysis}{7}}
\newlabel{sec:experiments}{{4}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Synthetic Datasets}{7}}
\newlabel{sec:synthetic-model}{{4.1}{7}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Optimized implementation of LBGA. Note that $1_E$ denotes the characteristic function of the event $E$.\relax }}{8}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:nef}{{1}{8}}
\citation{Erdos60}
\citation{Ley02}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Description of datasets analyzed. Total number of vertices in each synthetic source graph is $n=500$. $m$ is the number of graph sources. $k$ is the number of clusters. $n_i$ represents number of vertices in cluster $i$. $p_i$ and $r_i$ represent the within- and across-cluster edge probability for each the $m$ graph sources.\relax }}{9}}
\newlabel{datasets}{{1}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Real Datasets}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}DBLP}{9}}
\citation{RealityMining}
\citation{EnronConf}
\citation{Enron}
\citation{mallet}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}RealityMining}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Enron}{10}}
\citation{Danon05}
\citation{Newman06}
\citation{Leskovec2008}
\citation{Gleich2012}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Validation Procedure}{11}}
\newlabel{sec:validation}{{4.3}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Left: the results of LBGA on the RealityMining dataset. Right: the input graph of Bluetooth scan events. LBGA was run with $consistentNO$, $\nu = \varepsilon = 0.2$, $\delta = 0.05$\relax }}{12}}
\newlabel{fig:reality-mining-comparison}{{1}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Left: the results of LBGA on the Enron dataset. Right: the input graph of topic models thresholded at 0.9. LBGA was run with $consistentNO$, $\nu = \varepsilon = 0.2$, $\delta = 0.05$\relax }}{12}}
\newlabel{fig:enron-comparison}{{2}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Experimental Results}{12}}
\newlabel{sec:results}{{4.4}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Synthetic}{12}}
\citation{RealityMining}
\citation{Caceres2011}
\citation{Mccallum05}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}DBLP}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}RealityMining}{13}}
\citation{Fortunato07}
\citation{nadakuditi2012}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.4}Enron}{14}}
\newlabel{sec:enron-results}{{4.4.4}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.5}Comments}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Robustness and Scalability}{14}}
\newlabel{sec:additional-analysis}{{5}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Sensitivity Analysis}{14}}
\newlabel{sec:sensitivity-analysis}{{5.1}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Graph representation learning for LSBM-3. The LBGA parameters are $\varepsilon =\nu =0.2, \delta =0.05$. Plots in order top to bottom: 1. NMI of $A(G_t)$ with the ground truth clustering, 2. modularity of $G_t$ w.r.t $A(G_t)$, with the horizontal line showing the modularity of the union of the input graphs w.r.t. ground truth, 3. the number of edges in $G_t$, 4. the average probability weight (quality) of vertex pairs for $H_i$. The Erd\H {o}s-R\'{e}nyi graph converges to low weight by round 300.\relax }}{15}}
\newlabel{fig:local-sbm}{{3}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Scalability Analysis}{15}}
\newlabel{sec:scalability-analysis}{{5.2}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Aggregation of co-authorship (red curve) and title similarity graphs (green curve) for DBLP dataset.\relax }}{16}}
\newlabel{fig:dblp}{{4}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {6}A convergence theorem}{16}}
\newlabel{sec:convergence-theorem}{{6}{16}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces LBGA performance results. All datasets in this table were run with EC and $consistentNO$ using $\varepsilon = \nu = 0.2, \delta = 0.05$. Union modularity and conductance for real datasets was computed with the walktrap clustering. The order of edge type weights for the real datasets are: DBLP (coauthorship, title similarity); RealityMining (bluetooth, phone calls, cell tower proximity, reported friendship, in-lab proximity, out-lab proximity); Enron (email, topic similarity).\relax }}{17}}
\newlabel{EC_NO}{{2}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Performance of LBGA (measured by NMI) as a function of SNR for the LSBM model with different probabilities $p_i$ for the edge consistency measure (top) and $consistentNO$ (bottom).\relax }}{18}}
\newlabel{fig:sensitivity-analysis}{{5}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The runtime of LBGA as a function of number of edges on the union of input graphs.\relax }}{19}}
\newlabel{fig:scalability-analysis}{{6}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions}{19}}
\newlabel{sec:conclusion}{{7}{19}}
\citation{Adamic01}
\bibstyle{plain}
\bibdata{graphLearning}
\bibcite{Enron}{1}
\bibcite{Adamic01}{2}
\bibcite{Aggarwal2011}{3}
\bibcite{Arora12}{4}
\bibcite{Balcan2006}{5}
\bibcite{Balcan2008}{6}
\bibcite{Berlingerio2011}{7}
\@writefile{toc}{\contentsline {section}{\numberline {8}Acknowledgements}{20}}
\bibcite{Bubeck12}{8}
\bibcite{Bubeck09}{9}
\bibcite{CCK14}{10}
\bibcite{Caceres2011}{11}
\bibcite{Cai2005}{12}
\bibcite{Danon05}{13}
\bibcite{Dice1945}{14}
\bibcite{RealityMining}{15}
\bibcite{Erdos60}{16}
\bibcite{Fortunato07}{17}
\bibcite{Gallagher2008}{18}
\bibcite{Getoor2005}{19}
\bibcite{Gilbert2009}{20}
\bibcite{Gleich2012}{21}
\bibcite{Jaccard1912}{22}
\bibcite{EnronConf}{23}
\bibcite{Leskovec2008}{24}
\bibcite{Ley02}{25}
\bibcite{Mccallum05}{26}
\bibcite{mallet}{27}
\bibcite{Miller2014}{28}
\bibcite{Mucha2010}{29}
\bibcite{nadakuditi2012}{30}
\bibcite{Neville2005}{31}
\bibcite{Newman06}{32}
\bibcite{Papalexakis2013}{33}
\bibcite{Walktrap}{34}
\bibcite{Rocklin2013}{35}
\bibcite{Rossi2012}{36}
\bibcite{Schapire90}{37}
\bibcite{Tang2012}{38}
\bibcite{Tang2009}{39}
\bibcite{Wang87}{40}
\bibcite{Xiang2010}{41}
