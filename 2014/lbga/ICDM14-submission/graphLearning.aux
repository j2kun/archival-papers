\relax 
\citation{Getoor2005}
\citation{Gallagher2008}
\citation{Neville2005}
\citation{Caceres2011}
\citation{Miller2014}
\citation{Aggarwal2011}
\citation{Leskovec2008}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\citation{Rossi2012}
\citation{Xiang2010}
\citation{Gilbert2009}
\citation{Papalexakis2013}
\citation{Tang2009}
\citation{Tang2012}
\citation{Mucha2010}
\citation{Berlingerio2011}
\citation{kolda2009}
\citation{Rocklin2013}
\citation{Cai2005}
\citation{Balcan2006}
\citation{Balcan2008}
\citation{Schapire90}
\citation{Bubeck12}
\citation{Schapire90}
\citation{Bubeck09}
\citation{Arora12}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related work}{2}}
\newlabel{sec:related}{{II}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Representation learning and clustering}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Boosting and bandits}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}The Locally Boosted Graph Aggregation framework}{2}}
\newlabel{sec:lbga}{{III}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Framework details}{2}}
\newlabel{sec:framework}{{\unhbox \voidb@x \hbox {III-A}}{2}}
\citation{Walktrap}
\citation{Dice1945}
\citation{Jaccard1912}
\citation{Wang87}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Quality measures for community detection}{3}}
\newlabel{sec:quality-measures}{{\unhbox \voidb@x \hbox {III-B}}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}LBGA implementation}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experimental analysis}{3}}
\newlabel{sec:experiments}{{IV}{3}}
\citation{Erdos60}
\citation{Ley02}
\citation{RealityMining}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Optimized implementation of LBGA. Note that $1_E$ denotes the characteristic function of the event $E$.\relax }}{4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:nef}{{1}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Synthetic datasets}{4}}
\newlabel{sec:synthetic-model}{{\unhbox \voidb@x \hbox {IV-A}}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Description of datasets analyzed. Total number of vertices in each synthetic source graph is $n=500$. $m$ is the number of graph sources. $k$ is the number of clusters. $n_i$ represents number of vertices in cluster $i$. $p_i$ and $r_i$ represent the within- and across-cluster edge probability for each the $m$ graph sources.\relax }}{4}}
\newlabel{datasets}{{I}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Real datasets}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}1}DBLP}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}2}RealityMining}{4}}
\citation{EnronConf}
\citation{Enron}
\citation{mallet}
\citation{Danon05}
\citation{Newman06}
\citation{Leskovec2008}
\citation{Gleich2012}
\citation{Papalexakis2013}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}3}Enron}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}}Validation procedure}{5}}
\newlabel{sec:validation}{{\unhbox \voidb@x \hbox {IV-C}}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-D}}Experimental results}{5}}
\newlabel{sec:results}{{\unhbox \voidb@x \hbox {IV-D}}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {IV-D}1}Synthetic}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {IV-D}2}DBLP}{5}}
\citation{Mccallum05}
\citation{Fortunato07}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Graph representation learning for LSBM-3. The LBGA parameters are $\varepsilon =\nu =0.2, \delta =0.05$. Plots in order top to bottom: 1. NMI of $A(G_t)$ with the ground truth clustering, 2. modularity of $G_t$ w.r.t $A(G_t)$, with the horizontal line showing the modularity of the union of the input graphs w.r.t. ground truth, 3. the number of edges in $G_t$, 4. the average probability weight (quality) of vertex pairs for $H_i$. The Erd\H {o}s-R\'{e}nyi graph converges to low weight by round 300, even though it is initially favored. This is evidence of LBGA's ability to recover from initial bad luck.\relax }}{6}}
\newlabel{fig:local-sbm}{{1}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces LBGA with consistentNO on a dataset of Erd\H {o}s-R\'{e}nyi random graphs.\relax }}{6}}
\newlabel{fig:eronly}{{2}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Statistics about the aggregate graph produced by LBGA after 500 rounds on a suite of 4 Erd\H {o}s-R\'{e}nyi random graphs on 500 nodes and varying edge probability $p$.\relax }}{6}}
\newlabel{fig:erpvaries}{{3}{6}}
\newlabel{sec:enron-results}{{\unhbox \voidb@x \hbox {IV-D}3}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {IV-D}3}RealityMining \& Enron}{6}}
\citation{Papalexakis2013}
\citation{kolda2009}
\citation{Tang2009}
\citation{nadakuditi2012}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Left: the results of LBGA on the Enron dataset. Right: the input graph of topic models thresholded at 0.9. LBGA was run with $consistentNO$, $\nu = \varepsilon = 0.2$, $\delta = 0.05$\relax }}{7}}
\newlabel{fig:enron-comparison}{{4}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-E}}Comparison with GraphFuse}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Robustness and scaling}{7}}
\newlabel{sec:additional-analysis}{{V}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-A}}LBGA does not boost noise}{7}}
\newlabel{sec:boostnoise}{{\unhbox \voidb@x \hbox {V-A}}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-B}}Sensitivity analysis}{7}}
\newlabel{sec:sensitivity-analysis}{{\unhbox \voidb@x \hbox {V-B}}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Performance of LBGA (measured by NMI) as a function of SNR for the LSBM model with different probabilities $p_i$ for $consistentNO$.\relax }}{7}}
\newlabel{fig:sensitivity-analysis}{{5}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-C}}Scalability analysis}{7}}
\newlabel{sec:scalability-analysis}{{\unhbox \voidb@x \hbox {V-C}}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces LBGA performance results, compared to GraphFuse and a baseline union aggregation. All datasets in this table were run with $consistentNO$ using $\varepsilon = \nu = 0.2, \delta = 0.05$. Union modularity and conductance for real datasets was computed with the walktrap clustering. The order of edge type weights for the real datasets are: DBLP (coauthorship, title similarity); RealityMining (bluetooth, phone calls, cell tower proximity, reported friendship, in-lab proximity, out-lab proximity); Enron (email, topic similarity).\relax }}{8}}
\newlabel{EC_NO}{{II}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The runtime of LBGA.\relax }}{8}}
\newlabel{fig:scalability-analysis}{{6}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}A convergence theorem}{8}}
\newlabel{sec:convergence-theorem}{{VI}{8}}
\newlabel{thm:probabilistic-oracle}{{1}{9}}
\newlabel{cor:probabilistic-cor}{{1}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Conclusions}{9}}
\newlabel{sec:conclusion}{{VII}{9}}
\bibstyle{plain}
\bibdata{graphLearning}
\bibcite{Enron}{1}
\bibcite{Aggarwal2011}{2}
\bibcite{Arora12}{3}
\bibcite{Balcan2006}{4}
\bibcite{Balcan2008}{5}
\bibcite{Berlingerio2011}{6}
\bibcite{Bubeck12}{7}
\bibcite{Bubeck09}{8}
\bibcite{Caceres2011}{9}
\bibcite{Cai2005}{10}
\bibcite{Danon05}{11}
\bibcite{Dice1945}{12}
\bibcite{RealityMining}{13}
\bibcite{Erdos60}{14}
\bibcite{Fortunato07}{15}
\bibcite{Gallagher2008}{16}
\bibcite{Getoor2005}{17}
\bibcite{Gilbert2009}{18}
\bibcite{Gleich2012}{19}
\bibcite{Jaccard1912}{20}
\bibcite{EnronConf}{21}
\bibcite{kolda2009}{22}
\bibcite{Leskovec2008}{23}
\bibcite{Ley02}{24}
\bibcite{Mccallum05}{25}
\bibcite{mallet}{26}
\bibcite{Miller2014}{27}
\bibcite{Mucha2010}{28}
\bibcite{nadakuditi2012}{29}
\bibcite{Neville2005}{30}
\bibcite{Newman06}{31}
\bibcite{Papalexakis2013}{32}
\bibcite{Walktrap}{33}
\bibcite{Rocklin2013}{34}
\bibcite{Rossi2012}{35}
\bibcite{Schapire90}{36}
\bibcite{Tang2012}{37}
\bibcite{Tang2009}{38}
\bibcite{Wang87}{39}
\bibcite{Xiang2010}{40}
\@writefile{toc}{\contentsline {section}{\numberline {VIII}Acknowledgements}{10}}
\@writefile{toc}{\contentsline {section}{References}{10}}
