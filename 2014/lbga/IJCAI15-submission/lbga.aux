\relax 
\citation{Leskovec2008}
\citation{Getoor2005}
\citation{Caceres2011}
\citation{Miller2014}
\citation{Aggarwal2011}
\citation{Leskovec2008}
\citation{Rossi2012}
\citation{Xiang2010}
\citation{Gilbert2009}
\citation{Wang14}
\citation{Papalexakis2013}
\citation{Tang2009}
\citation{Tang2012}
\citation{Mucha2010}
\citation{Berlingerio2011}
\citation{kolda2009}
\citation{Shiga12}
\citation{Papalexakis2013}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related work}{1}}
\newlabel{sec:related}{{2}{1}}
\newlabel{cref@sec:related}{{[section][2][]2}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Graph representation learning and clustering}{1}}
\citation{Rocklin2013}
\citation{Cai2005}
\citation{Schapire90}
\citation{Bubeck12}
\citation{Schapire90}
\citation{Bubeck09}
\citation{Arora12}
\citation{Walktrap}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Boosting and bandits}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}The Locally Boosted Graph Aggregation framework}{2}}
\newlabel{sec:lbga}{{3}{2}}
\newlabel{cref@sec:lbga}{{[section][3][]3}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Framework details}{2}}
\newlabel{sec:framework}{{3.1}{2}}
\newlabel{cref@sec:framework}{{[subsection][1][3]3.1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Quality measures for community detection}{2}}
\newlabel{sec:quality-measures}{{3.2}{2}}
\newlabel{cref@sec:quality-measures}{{[subsection][2][3]3.2}{2}}
\citation{Dice1945}
\citation{Wang87}
\citation{Erdos60}
\citation{Ley02}
\citation{RealityMining}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}LBGA implementation}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental analysis}{3}}
\newlabel{sec:experiments}{{4}{3}}
\newlabel{cref@sec:experiments}{{[section][4][]4}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Synthetic datasets}{3}}
\newlabel{sec:synthetic-model}{{4.1}{3}}
\newlabel{cref@sec:synthetic-model}{{[subsection][1][4]4.1}{3}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Optimized implementation of LBGA. Note that $1_E$ denotes the characteristic function of the event $E$.\relax }}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:nef}{{1}{3}}
\newlabel{cref@alg:nef}{{[algocf][1][]1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Real datasets}{3}}
\citation{EnronConf}
\citation{mallet}
\citation{Danon05}
\citation{Newman06}
\citation{Leskovec2008}
\citation{Papalexakis2013}
\citation{Wang14}
\citation{Mccallum05}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Description of datasets analyzed. Total number of vertices in each synthetic source graph is $n=500$. The number of graph sources is $m$. The number of clusters is $k$. The number of vertices in cluster $i$ is $n_i$. The within- and across-cluster edge probabilities for graph source $i$ are $p_i$ and $r_i$, respectively.\relax }}{4}}
\newlabel{datasets}{{1}{4}}
\newlabel{cref@datasets}{{[table][1][]1}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Validation procedure}{4}}
\newlabel{sec:validation}{{4.3}{4}}
\newlabel{cref@sec:validation}{{[subsection][3][4]4.3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Experimental results}{4}}
\newlabel{sec:results}{{4.4}{4}}
\newlabel{cref@sec:results}{{[subsection][4][4]4.4}{4}}
\@writefile{toc}{\contentsline {subparagraph}{Synthetic}{4}}
\@writefile{toc}{\contentsline {subparagraph}{DBLP}{4}}
\@writefile{toc}{\contentsline {subparagraph}{RealityMining \& Enron}{4}}
\newlabel{sec:enron-results}{{4.4}{4}}
\newlabel{cref@sec:enron-results}{{[figure][2][]2}{4}}
\citation{Fortunato07}
\citation{Papalexakis2013}
\citation{Wang14}
\citation{nadakuditi2012}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Graph representation learning for LSBM-3. The LBGA parameters are $\varepsilon =\nu =0.2, \delta =0.05$. Plots in order top to bottom: 1. NMI of $A(G_t)$ with the ground truth clustering, 2. modularity of $G_t$ w.r.t. $A(G_t)$, with the horizontal line showing the modularity of the union of the input graphs w.r.t. ground truth, 3. the number of edges in $G_t$, 4. the average probability weight of vertex pairs for $H_i$. The Erd\H {o}s-R\'{e}nyi graph converges to low weight by round 300, even though it is initially favored. Hence LBGA can recover from bad luck and does not boost noise.\relax }}{5}}
\newlabel{fig:local-sbm}{{1}{5}}
\newlabel{cref@fig:local-sbm}{{[figure][1][]1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Comparison with GraphFuse and SNF}{5}}
\newlabel{sec:comparison}{{4.5}{5}}
\newlabel{cref@sec:comparison}{{[subsection][5][4]4.5}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Sensitivity analysis}{5}}
\newlabel{sec:sensitivity-analysis}{{4.6}{5}}
\newlabel{cref@sec:sensitivity-analysis}{{[subsection][6][4]4.6}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Statistics about the aggregate graph produced by LBGA after 500 rounds on a suite of 4 Erd\H {o}s-R\'{e}nyi random graphs on 500 nodes and varying edge probability $p$.\relax }}{5}}
\newlabel{fig:erpvaries}{{2}{5}}
\newlabel{cref@fig:erpvaries}{{[figure][2][]2}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Left: the results of LBGA on the Enron dataset. Right: the input graph of topics.\relax }}{5}}
\newlabel{fig:enron-comparison}{{3}{5}}
\newlabel{cref@fig:enron-comparison}{{[figure][3][]3}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{5}}
\newlabel{sec:conclusion}{{5}{5}}
\newlabel{cref@sec:conclusion}{{[section][5][]5}{5}}
\bibstyle{plain}
\bibdata{lbga}
\bibcite{Aggarwal2011}{1}
\bibcite{Arora12}{2}
\bibcite{Berlingerio2011}{3}
\bibcite{Bubeck12}{4}
\bibcite{Bubeck09}{5}
\bibcite{Caceres2011}{6}
\bibcite{Cai2005}{7}
\bibcite{Danon05}{8}
\bibcite{Dice1945}{9}
\bibcite{RealityMining}{10}
\bibcite{Erdos60}{11}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces LBGA performance results, compared to GraphFuse, SMF, and a baseline union aggregation. All datasets in this table were run with $consistentNO$ using $\varepsilon = \nu = 0.2, \delta = 0.05$. Union modularity and conductance for real datasets was computed with the walktrap clustering. Values were averaged over 10 trials, and when variances $\sigma ^2 > 10^{-4}$ were observed, values are reported with $\pm \sigma $.\relax }}{6}}
\newlabel{table:results}{{2}{6}}
\newlabel{cref@table:results}{{[table][2][]2}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Performance of LBGA (measured by NMI) as a function of SNR for the LSBM model with different probabilities $p_i$ for $consistentNO$.\relax }}{6}}
\newlabel{fig:sensitivity-analysis}{{4}{6}}
\newlabel{cref@fig:sensitivity-analysis}{{[figure][4][]4}{6}}
\bibcite{Fortunato07}{12}
\bibcite{Getoor2005}{13}
\bibcite{Gilbert2009}{14}
\bibcite{EnronConf}{15}
\bibcite{kolda2009}{16}
\bibcite{Leskovec2008}{17}
\bibcite{Ley02}{18}
\bibcite{Mccallum05}{19}
\bibcite{mallet}{20}
\bibcite{Miller2014}{21}
\bibcite{Mucha2010}{22}
\bibcite{nadakuditi2012}{23}
\bibcite{Newman06}{24}
\bibcite{Papalexakis2013}{25}
\bibcite{Walktrap}{26}
\bibcite{Rocklin2013}{27}
\bibcite{Rossi2012}{28}
\bibcite{Schapire90}{29}
\bibcite{Shiga12}{30}
\bibcite{Tang2012}{31}
\bibcite{Tang2009}{32}
\bibcite{Wang14}{33}
\bibcite{Wang87}{34}
\bibcite{Xiang2010}{35}
