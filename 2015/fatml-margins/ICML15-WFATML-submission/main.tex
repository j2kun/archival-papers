\documentclass{article}
\usepackage{icml2015}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}

\hyphenation{AdaBoost}

\title{Fair boosting: a case study}
%\author{Benjamin Fish, Jeremy Kun, \'Ad\'am Lelkes}
\author{Submitted for blind review}

\DeclareMathOperator{\margin}{margin}
\DeclareMathOperator{\sign}{sign}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}

\begin{document}

\maketitle

\begin{abstract} 

% We study the fairness properties of the classical AdaBoost algorithm
% of~\cite{FreundS97} in the context of the Census Income
% Dataset~\cite{UCIAdult}.
We study the classical AdaBoost algorithm in the context of fairness.
We use the Census Income Dataset~\citep{UCIAdult} as a case study.
We empirically evaluate the bias and error of four %!FIVE WITH MTR
variants of AdaBoost relative to an unmodified AdaBoost baseline, and study the
trade-offs between reducing bias and maintaining low error. We further define a 
new notion of fairness and measure it for all of our methods.  Our
proposed method, modifying the hypothesis output by AdaBoost by ``shifting the
margin'' for the protected group, outperforms the state of the art for the census
dataset.

\end{abstract}

Although there are several papers on ``fair'' versions of learning algorithms
such as naive Bayes, decision tree learning or logistic regression, boosting,
which is one of the most successful and most widely used machine learning
algorithms, has not been studied in the context of fair learning before. In
addition to its popularity, boosting is an interesting framework in which to
study fairness because notions such as a weak learner and the boosting margin
have natural interpretations for fairness. We rigorously define these notions
in Section~\ref{sec:methods} and analyze them in Section~\ref{sec:results}.

Following previous literature, we assume that the training data is biased
against data points with a given feature value but we do not have access to the
unbiased ground truth. We want to learn a classifier which has minimal error
(as evaluated on the biased data) among all classifiers that achieve
statistical parity.  \citet{DworkHPR12} point out that bias represents
a notion of group fairness rather than individual fairness, and that
it is still possible to discriminate against individuals even when
achieving statistical parity.  Thus, in addition to learning a classifier
that has both low bias and error, we want a classifier that performs well
on a measure of individual fairness.
In this paper, we introduce a notion of fairness that captures how resistant
a classifier is to uniform random noise on the training labels of the data points
with a given feature value.
 
The Census Income Data Set~\citep{UCIAdult} is a widely used data set for
machine learning research in which the learner's goal is to predict whether an
individual's income exceeds \$50k per year based on census data such as age,
education, gender, and marital status. In particular, when considering gender
as a protected attribute, the dataset exhibits high bias. We use this data set
as a case study to understand the fairness properties of the AdaBoost algorithm
of~\citet{FreundS97}. We review the boosting algorithm and provide information
about the Census data set in Section~\ref{sec:background}.

A primary advantage to using boosting is that boosting has a natural notion of
margin which we can take advantage of to try to decrease bias while keeping error low.
Our main empirical finding is that after boosting is performed to produce a
hypothesis $h$, flipping the output label of $h$ according to the boosting
margin of the protected group outperforms the state of the art on the Census
dataset both in terms of bias and label error. We compare this to data
massaging (introduced by~\citet{KamiranC09}), replacing a standard weak learner with
a ``fair'' weak learner, and uniform random relabeling. Finally, in
Section~\ref{sec:discussion} we interpret and discuss our results. 

\section{Background} \label{sec:background}

\subsection{Notions of fairness} The study of fairness in machine learning is
still young, and to the best of our knowledge we are the first to study the
fairness properties of boosting.  There are two prominent definitions of
``fairness'' that have been studied in the literature. The first is
\emph{discrimination} or \emph{bias}, which for a distribution $D$ over a set
of labeled examples $X$ with label $l : X \to \{-1, 1\}$ and a protected subset
$S \subset X$ is defined as the difference in probability of an example in $S$
having label 1 and the probability of an example in $S^C$ having label 1, i.e.
$$ B(X, D, S) = \left | \Pr_{x \sim D|_{S^C}}[l(x) = 1] - \Pr_{x \sim D|_{S}}
[l(x) = 1]  \right |. $$ Similarly the bias of a hypothesis $h$ is the same
quantity with $h(x)$ replacing $l(x)$. If a hypothesis has low bias we say it
achieves \emph{statistical parity.}  $S$ represents the group we wish to protect
from discrimination, and the bias represents the degree to which they have been
discriminated against.

\citet{DworkHPR12} point out that while bias is undesirable, it does not account
for all possible forms of unfairness --- it is a measure of group fairness
rather than individual fairness.

The second notion, due to~\citet{DworkHPR12},
measures individual fairness and requires a metric on the underlying set $X$.
They call a learning algorithm ``individually fair'' if the output of the
learning algorithm is similar for individuals which are close in $X$. 

In this light, we define a new notion of fairness that departs from
previous literature in that it does not require a metric on the underlying
space. Rather, it makes the assumption that the process generating the bias is
uniformly random, and measures the ability for an algorithm to recover the true
labels from the biased dataset.
% We also define \emph{resistance to uniform
% bias} which measures the degree to which bias in a training set is encoded in a
% hypothesis which is then run on unbiased test data.
% Finally, all of our methods
% that try to optimize for ``fairness'' specifically optimize for statistical
% parity.  We then measure the fairness properties of the resulting hypotheses. 

\subsection{AdaBoost}

Boosting algorithms work by combining \emph{base hypotheses}, ``rules of thumb'' that
are barely more accurate than random guessing, into highly accurate predictors.
On each round, a boosting algorithm will change the weights of the data points
and find the base hypothesis that achieves the smallest weighted error on the sample.
It always increases the weights of the incorrectly classified examples, thus forcing
the base learner to improve the classification of the examples that are the hardest
to classify correctly. In this paper, we focus on AdaBoost, the most ubiquitous 
boosting algorithm. The algorithm is given in Algorithm~\ref{adaboost}. In all
of our experiments we boost decision stumps for $T=20$ rounds
(after which accuracy does not significantly improve).

\begin{algorithm}
\caption{AdaBoost \citep{FreundS97}}
\begin{algorithmic}\label{adaboost}
\FOR {$i=1$ \TO $m$}
\STATE $D_1(i) = \frac1m$
\ENDFOR
\FOR {$t=1$ \TO $T$}
\STATE $h_t = $ base hypothesis with smallest error
\STATE $\epsilon_t = \sum_{i=1}^m D_t(i) (1-\delta_{h_t(x_i), y_i})$
\STATE $\alpha_t = \frac12\log\frac{1-\epsilon_t}{\epsilon_t}$
\STATE $Z_t = 2\sqrt{\epsilon_t (1-\epsilon_t)}$
\FOR {$i=1$ \TO $m$}
\STATE $ D_{t+1}(i) = \frac{D_t(i) e^{-h_t(x_i) y_i}}{Z_t}$
\ENDFOR
\ENDFOR
\STATE $g=\sum_{t=1}^T \alpha_t h_t$
\STATE $h = \mathrm{sgn}\circ g$
\RETURN $h$
\end{algorithmic}
\end{algorithm}

Given hypotheses $h_i$ with weights $\alpha_i$ computed by AdaBoost, we call the
\emph{margin} of $x \in X$ is $$ \margin(x) = \frac{\sum_i \alpha_i
h_i(x)}{\sum_i \alpha_i}.$$
Note that this is different from the usual definition of the margin: in this paper,
we do not multiply the combination of the base hypothesis by the correct label.
The reason is that we want a measure of ``confidence'' of AdaBoost which can be
computed by the learning algorithm even when it does not have access to the
correct labels.
Tt is well known that examples with small
margins in magnitude contribute to most of the error of
AdaBoost~\citep{SchapireFBL98}.  As such, flipping small-margin examples should
maintain low error, and we show in our experiments that this is indeed the
case.

\subsection{Baseline statistics about the Census dataset}

The Census Income dataset, extracted from the 1994 Census database,
contains demographic information about $48842$ American adults.
The prediction task is to determine whether a person earns over \$50K
a year. $16192$ of the people in the dataset are female, $32650$
are male. $30.38\%$ of men and $10.93\%$ of women earn more than \$50K,
therefore the bias of the dataset is $19.45\%$.

AdaBoost achieves an error of $15\%$ after $20$ rounds of boosting.
The bias of the classifier output by vanilla AdaBoost is $18\%$.
We note that simply removing the protected feature from the data
does not reduce bias at all in this case since 
the classifier output by vanilla AdaBoost
trained for $20$ rounds on the full data doesn't (explicitly) use gender.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{images/censusmargins.pdf}
\caption{Histogram of boosting margins for the Census data set. The vast
majority of women are classified as -1, and the incorrect classifications are
closer to the decision boundary.}
\label{fig:boosting-margins}
\end{figure}

\section{Methods} \label{sec:methods}

We define our methods. In what follows $X$ is a labeled dataset,
$l(x)$ are the ground truth labels, and $S \subset X$ is the protected group.
We further assume that members of $S$ are less likely than $S^C$ to have label
1, which is true of the Census dataset when $S$ is the women, for example.
First we describe three relabeling algorithms.  
A relabeling algorithm, when given a hypothesis $h$ and a labeled data set $X, l$,
produces a new hypothesis $h'$ that uses $h$ as a black box and flips the output
of $h$ according to some rule.

The \emph{uniform random relabeling} (RR) algorithm computes the probability
$p$ for which, if members of $S$ with label $-1$ under $h$ are flipped by $h'$ to $+1$,
the bias of $h'$ is zero in expectation.  $h'$ is then the randomized classifier that
flips members of $S$ with label $-1$ with probability $p$ and otherwise is the same as $h$.

% The \emph{margin-threshold relabeling} (MTR) algorithm computes the value
% $\tau$ such that if all labels of examples in $S$ with $|\margin(x)| < \tau$
% are flipped by $h'$ then the bias of $h'$ on $X$ is minimized. Then $h'$ is
% defined as $-h(x)$ when $x \in S, |\margin(x)| < \tau$ and $h(x)$ otherwise.
% 
% %One might ask that if the goal is to reduce bias, why are positive labels with
% %margins under the threshold flipped as well as negative ones? We empirically
% %observe the margins of the protected class are not symmetrically distributed
% %around the decision boundary. Thus by flipping all labels with margins under
% %the threshold, we reduce bias without significantly increasing error.
% Margin-threshold relabeling flips labels symmetrically about the decision boundary.
% The following can be though of as a one-sided version of the margin-threshold relabeling,
% that only flips labels from -1 to 1 that have small margin:

The \emph{margin-shift relabeling} (MSR) algorithm computes the value $\theta$
such that bias is minimized by ``shifting the margin'' for examples from $S$
from zero to $\theta$. That is, $x \in S$ then $h'(x) = 1$ iff $\margin(x) >=
\theta$, and otherwise $h'(x) = \sign(\margin(x))$ as usual. 

Next, we define \emph{random massaging} (RM).
Massaging strategies, introduced by~\citet{KamiranC09}, involve eliminating the bias of
the training data by modifying the labels of data points, and then training a
classifier on this data in the hope that the statistical parity of the training data
will generalize to the test set as well.
In our experiment, we massage the data randomly; i.e.~we flip the labels of $S$ from $+1$ to $-1$ independently at random
to achieve statistical parity in expectation.

Finally, in \emph{fair weak learning} (FWL) we replace a standard boosting weak
learner with one which tries to minimize a linear combination of error and bias
and run the resulting boosting algorithm unchanged. The weak learner we used
computed the decision stump which minimizes the sum of label error and bias of
its induced hypothesis.

To measure fairness, we test how these algorithms are resistant to uniform random noise
that introduces bias against a random subset of the individuals.
This is formalized as follows:

% \begin{definition}
% We define the \emph{resistance to uniform bias} (RUB) of a learning algorithm
% $A$ on a dataset $X,l$ as follows. Introduce a new uniform random binary
% feature $z$ on elements of $X$. Split $X$ into training and testing subsets
% $X_{\textup{train}}, X_{\textup{test}}$ and flip the positive labels of
% examples $x \in X_{\textup{train}}$ that have $z=1$ independently with
% probability $p$ to $-1$. Run $A$ on $X_{\textup{train}}$ and let $h$ be the
% resulting hypothesis. The resistance of $A$ is the accuracy of $h$ on the
% subset of $X_{\textup{test}}$ (with the original, unbiased labels)
% corresponding to the protected group $z=1$.  
% \end{definition}

\begin{definition}
We define the \emph{uniform bias individual fairness} (UBIF) of a learning
algorithm $A$ on a labeled dataset $X,l$ as follows. Introduce a new uniform
random binary feature $z$ on elements of $X$. Flip the labels of examples $x$
that have $z=0$ independently with probability $p$ to $-1$ to get a new dataset $X',
l'$. Run $A$ on $X', l'$ and let $h$ be the resulting hypothesis. The
uniform bias individual fairness of $A$ is the fraction of flipped examples $x
\in X'$ for which $h(x) = l(x)$. 
\end{definition}

In our experiments we set $p = 0.2$.  UBIF can be thought of as the following experiment:  
A learning algorithm is given a dataset in which bias has been generated uniformly at random.  That is, we change the labels of a few individuals based on a feature which is blatantly random with respect to the classification task.  For example, we purposefully flip a few labels in the data set of individuals who prefer chocolate ice cream over vanilla ice cream.  The goal
of an algorithm is to then recover the ground truth labels in the original
dataset, recovering from the egregious bias against chocolate ice cream lovers.  This models the ability of the algorithm to recover from bias against a few individuals.

\section{Results} \label{sec:results}

In this section we state our experimental results. They are summarized in
Table~\ref{table:results}. We also included the numbers for the Learning Fair
Representations method of~\citet{ZemelWSPD13}. In that paper, the authors
implemented three other learning algorithms, these are unregularized logistic
regression, Fair Naive-Bayes \citep{KamiranC09}, and Regularized Logistic
Regression \citep{KamashimaAS11}. These methods all had errors above $20\%$;
thus we see that our margin-based relabeling methods outperform the state of
the art.  To investigate the trade-offs made by these relabeling methods more
closely, Figure~\ref{fig:tradeoffs} shows the rate at which error increases as bias goes to zero.

\begin{table*}
\centering
\begin{tabular}{| c | ccccccc |}
\hline
               & AdaBoost & RR   %& MTR  
               & MSR  & RM   & FWL  & LFR \cite{ZemelWSPD13} \\
\hline 
label error    & 0.15     & 0.20 %& 0.20 
& 0.18 & 0.18 & 0.18 & 0.25 \\ 
bias           & 0.18     & 0.00 %& 0.00
& 0.00 & 0.03 & 0.07 & 0.00 \\ 
% RUB            & 0.84     & 0.81 &  & 0.85 & 0.85 & 0.84 & - \\
UBIF           & 0.44     & 0.46   & 0.54 
& 0.54 & 0.48  & n/a \\ 
\hline 
\hline 
\end{tabular}
\caption{A summary of our experimental results for relabeling, massaging, and
the fair weak learner. The threshold for margin threshold relabeling (MTR) and
the shift for margin shift relabeling (MSR) were chosen to make statistical
parity zero.}
\label{table:results}
\end{table*}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{images/relabeling-msr-tradeoffs.pdf}
\caption{Trade-off between bias and error for margin shifting. The horizontal axis is the shift used for MSR.}
\label{fig:tradeoffs}
\end{figure}

\section{Discussion}\label{sec:discussion}

Margin shift relabeling (MSR) performs equally to or outperforms every proposed
method on all measures of bias and error, including the previous state of the
art for the census dataset. Indeed, there is significant theoretical justification
that shifting the decision boundary for the protected group achieves relatively high levels of
fairness.  While it is always possible to shift the decision boundary until statistical parity is achieved, the risk is that some of the data points with changed labels are now labeled incorrectly, increasing error.  For example, when the data points that are relabeled are chosen randomly, as in RR, each is now likely to be misclassified, resulting in an additional $5$ percent error as seen in Table~\ref{table:results}.  To decrease error, we want to find the data points whose labels boosting was the most unsure of since 
these are more likely to be classified incorrectly by boosting.
This means we should choose the data points to relabel with the smallest margin, as in MSR.
Figure~\ref{fig:boosting-margins} shows that the
distribution of margins for women is noticeably shifted when compared to the
whole population, giving empirical evidence that this approach is sensible.

Of course, how data points with small margin are relabeled does matter.  If it is done symmetrically so that both points labeled $-1$ and $1$ are flipped, then it takes a larger threshold to achieve statistical parity when compared to MSR, which only flips labels from $-1$ to $1$.  This means fewer points need to be flipped, which in turn decreases error when compared to a symmetric version.  It outperforms the baseline RR, where margin is not considered, showing that the points with small margin (in absolute value) are indeed less likely to be labeled correctly than points with large margin.

%Margin threshold relabeling also performs well, though as we see in
%Figure~\ref{fig:tradeoffs} that the tradeoff between bias and label error
%favors margin shift relabeling by a nontrivial margin. In addition, margin
%shifting has better individual fairness compared to margin threshold
%relabeling, which we expect because flipping all labels close to the margin
%is unfair to those individuals who are classified positively and should be so. 

Replacing a standard weak learner by a weak learner that tries to minimize a combination of error and bias (FWL) does empirically reduce bias, but does not quite achieve statistical parity. Moreover, the label error of FWL is not better than that of MSR, and the trade-off between label error and bias cannot easily be controlled. The same is true for random massaging (RM).

A natural baseline for UBIF is $0.5$, since a hypothesis chosen uniformly at random will flip back half of the points that were flipped to $-1$.  Unmodified AdaBoost encodes the bias introduced, performing worse than $0.5$.  The question is whether we can recover from this randomly introduced bias, while still achieving low label error and low overall bias.  Under RR, UBIF increases marginally, as it randomly flips labels back to a label of $1$, a few of which were the points randomly biased against.  MSR performs better than RR, indicating that the points that were randomly biased against have small margin under boosting.

A further advantage of MSR is that the trade-off between label error and bias can be controlled after training.
To decide how much bias and error we want to allow, we do not have to fix the value of a hyperparameter before training the algorithm,
unlike for most other fair learning methods. This means that the computational cost of choosing the best point on the trade-off curve is very low.

While these results are preliminary, they show the advantages of fair boosting:  the margin can be used to find a superior classifier.  We also give preliminary results that suggest the usefulness of measuring an algorithm's resistance to uniform bias.
%Random relabeling has poor error and
%little resistance to uniform bias, but it still outperforms non-boosting
%methods with respect to label error. Finally, random massaging achieves good
%label error and adequate statistical parity, ...

 




\newpage
\bibliographystyle{icml2015}
\bibliography{main}

\end{document}
